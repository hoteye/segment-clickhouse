# AI æ™ºèƒ½åˆ†ææ¨¡å—

## æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ AI æ™ºèƒ½åŒ–åº”ç”¨æ€§èƒ½åˆ†ææ¨¡å—ï¼ŒåŸºäº Spring Boot æ„å»ºï¼Œé›†æˆäº†å¤šç§ LLM æä¾›å•†ï¼Œèƒ½å¤Ÿå¯¹å¯è§‚æµ‹æ€§æ•°æ®è¿›è¡Œæ™ºèƒ½åˆ†æå¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚

## ä¸»è¦åŠŸèƒ½

### ğŸ” æ™ºèƒ½æ€§èƒ½åˆ†æ
- è‡ªåŠ¨æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼ˆJVMã€åº”ç”¨ã€æ•°æ®åº“ã€ç³»ç»Ÿèµ„æºï¼‰
- åŸºäºé˜ˆå€¼çš„å¼‚å¸¸æ£€æµ‹
- LLM é©±åŠ¨çš„æ™ºèƒ½åˆ†æå’Œæ´å¯Ÿ

### ğŸ¤– å¤š LLM æ”¯æŒ
- **OpenAI GPT**ï¼šæ”¯æŒ GPT-3.5/GPT-4
- **Azure OpenAI**ï¼šä¼ä¸šçº§ OpenAI æœåŠ¡
- **æœ¬åœ° LLM**ï¼šæ”¯æŒ Ollama ç­‰æœ¬åœ°éƒ¨ç½²æ¨¡å‹
- **é™çº§æ–¹æ¡ˆ**ï¼šLLM ä¸å¯ç”¨æ—¶çš„åŸºç¡€åˆ†æ

### ğŸ“Š æŠ¥å‘Šç”Ÿæˆ
- æ™ºèƒ½åŒ–åˆ†ææŠ¥å‘Š
- ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
- æŠ¥å‘Šå­˜å‚¨å’Œæ£€ç´¢
- å®šæ—¶è‡ªåŠ¨åˆ†æ

### ğŸ”§ REST API
- æŠ¥å‘Šç”Ÿæˆ API
- æŠ¥å‘ŠæŸ¥è¯¢ API
- åˆ†æä»»åŠ¡è§¦å‘
- å¥åº·æ£€æŸ¥

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚
- Java 11+
- Maven 3.6+
- ClickHouse æ•°æ®åº“
- ï¼ˆå¯é€‰ï¼‰OpenAI API Key æˆ–æœ¬åœ° LLM

### 2. é…ç½®

ç¼–è¾‘ `src/main/resources/application.yml`ï¼š

```yaml
# æ•°æ®æºé…ç½®
spring:
  datasource:
    url: jdbc:clickhouse://localhost:8123/default
    username: default
    password: 

# AI é…ç½®
ai:
  llm:
    provider: openai  # openai, azure, ollama
    openai:
      api-key: ${AI_OPENAI_API_KEY}
      model: gpt-3.5-turbo
```

### 3. ç¯å¢ƒå˜é‡

```bash
# OpenAI é…ç½®
export AI_OPENAI_API_KEY="your-openai-api-key"
export AI_OPENAI_BASE_URL="https://api.openai.com/v1"

# æˆ–è€… Azure OpenAI é…ç½®
export AI_AZURE_API_KEY="your-azure-api-key"
export AI_AZURE_ENDPOINT="https://your-resource.openai.azure.com"
export AI_AZURE_DEPLOYMENT="your-deployment-name"

# æˆ–è€…æœ¬åœ° LLM é…ç½®
export AI_LOCAL_LLM_URL="http://localhost:11434"
export AI_LOCAL_LLM_MODEL="llama2"
```

### 4. æ„å»ºå’Œè¿è¡Œ

```bash
# è¿›å…¥ AI æ¨¡å—ç›®å½•
cd ai-analysis-module

# æ„å»ºé¡¹ç›®
mvn clean package

# è¿è¡Œåº”ç”¨
java -jar target/ai-analysis-module-1.0.5.jar

# æˆ–è€…ç›´æ¥è¿è¡Œ
mvn spring-boot:run
```

åº”ç”¨å°†åœ¨ `http://localhost:8081` å¯åŠ¨

## API ä½¿ç”¨

### ç”Ÿæˆåˆ†ææŠ¥å‘Š

```bash
# ç”Ÿæˆè¿‡å»1å°æ—¶çš„æŠ¥å‘Š
curl -X POST "http://localhost:8081/ai-analysis/api/ai-analysis/reports/generate?timeRangeHours=1"

# ç”Ÿæˆè¿‡å»24å°æ—¶çš„æŠ¥å‘Š
curl -X POST "http://localhost:8081/ai-analysis/api/ai-analysis/reports/generate?timeRangeHours=24"
```

### è·å–æŠ¥å‘Šåˆ—è¡¨

```bash
# è·å–æœ€è¿‘10ä¸ªæŠ¥å‘Š
curl "http://localhost:8081/ai-analysis/api/ai-analysis/reports?limit=10"
```

### è·å–ç‰¹å®šæŠ¥å‘Š

```bash
# æ ¹æ®æŠ¥å‘ŠIDè·å–è¯¦ç»†ä¿¡æ¯
curl "http://localhost:8081/ai-analysis/api/ai-analysis/reports/{reportId}"
```

### å¥åº·æ£€æŸ¥

```bash
curl "http://localhost:8081/ai-analysis/api/ai-analysis/health"
```

## é…ç½®è¯´æ˜

### LLM æä¾›å•†é…ç½®

#### OpenAI
```yaml
ai:
  llm:
    provider: openai
    openai:
      api-key: ${AI_OPENAI_API_KEY}
      base-url: https://api.openai.com/v1
      model: gpt-3.5-turbo
      timeout: 30000
      max-tokens: 2000
      temperature: 0.7
```

#### Azure OpenAI
```yaml
ai:
  llm:
    provider: azure
    azure:
      api-key: ${AI_AZURE_API_KEY}
      endpoint: ${AI_AZURE_ENDPOINT}
      deployment-name: ${AI_AZURE_DEPLOYMENT}
      api-version: 2023-05-15
```

#### æœ¬åœ° LLM (Ollama)
```yaml
ai:
  llm:
    provider: ollama
    local:
      url: http://localhost:11434
      model: llama2
      timeout: 60000
```

### åˆ†æé…ç½®

```yaml
ai:
  analysis:
    enabled: true
    window:
      hours: 1  # é»˜è®¤åˆ†ææ—¶é—´çª—å£
    schedule:
      enabled: true
      cron: "0 0 */1 * * ?"  # æ¯å°æ—¶æ‰§è¡Œ
    thresholds:
      response-time-ms: 1000
      error-rate-percent: 5.0
      cpu-usage-percent: 80.0
      memory-usage-percent: 85.0
```

## é¡¹ç›®ç»“æ„

```
ai-analysis-module/
â”œâ”€â”€ src/main/java/com/o11y/ai/
â”‚   â”œâ”€â”€ AiAnalysisApplication.java          # å¯åŠ¨ç±»
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ AiAnalysisProperties.java       # é…ç½®å±æ€§
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â””â”€â”€ PerformanceAnalysisController.java  # REST API
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ PerformanceAnalysisService.java     # æ ¸å¿ƒåˆ†ææœåŠ¡
â”‚   â”‚   â”œâ”€â”€ LLMAnalysisService.java             # LLM åˆ†ææœåŠ¡
â”‚   â”‚   â””â”€â”€ ReportStorageService.java           # æŠ¥å‘Šå­˜å‚¨æœåŠ¡
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ PerformanceMetrics.java             # æ€§èƒ½æŒ‡æ ‡æ¨¡å‹
â”‚       â”œâ”€â”€ PerformanceAnomaly.java             # å¼‚å¸¸æ¨¡å‹
â”‚       â””â”€â”€ PerformanceReport.java              # æŠ¥å‘Šæ¨¡å‹
â”œâ”€â”€ src/main/resources/
â”‚   â””â”€â”€ application.yml                     # é…ç½®æ–‡ä»¶
â””â”€â”€ pom.xml                                 # Maven é…ç½®
```

## æ‰©å±•å’Œå®šåˆ¶

### æ·»åŠ æ–°çš„æŒ‡æ ‡æ”¶é›†å™¨

å®ç° `PerformanceAnalysisService` ä¸­çš„æŒ‡æ ‡æ”¶é›†æ–¹æ³•ï¼š

```java
private void collectCustomMetrics(Connection conn, PerformanceMetrics metrics, 
                                 LocalDateTime startTime, LocalDateTime endTime) {
    // è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†é€»è¾‘
}
```

### æ·»åŠ æ–°çš„å¼‚å¸¸æ£€æµ‹å™¨

åœ¨ `detectAnomalies` æ–¹æ³•ä¸­æ·»åŠ è‡ªå®šä¹‰å¼‚å¸¸æ£€æµ‹é€»è¾‘ï¼š

```java
// è‡ªå®šä¹‰å¼‚å¸¸æ£€æµ‹
if (customCondition) {
    PerformanceAnomaly anomaly = new PerformanceAnomaly();
    // è®¾ç½®å¼‚å¸¸å±æ€§
    anomalies.add(anomaly);
}
```

### é›†æˆæ–°çš„ LLM æä¾›å•†

åœ¨ `LLMAnalysisService` ä¸­æ·»åŠ æ–°çš„è°ƒç”¨æ–¹æ³•ï¼š

```java
private String callCustomLLM(String prompt) throws Exception {
    // è‡ªå®šä¹‰ LLM è°ƒç”¨é€»è¾‘
}
```

## ç›‘æ§å’Œè¿ç»´

### æ—¥å¿—

æ—¥å¿—æ–‡ä»¶ä½ç½®ï¼š`logs/ai-analysis.log`

å…³é”®æ—¥å¿—çº§åˆ«ï¼š
- `INFO`ï¼šæ­£å¸¸æ“ä½œæ—¥å¿—
- `WARN`ï¼šè­¦å‘Šä¿¡æ¯ï¼ˆå¦‚ LLM è°ƒç”¨å¤±è´¥ï¼‰
- `ERROR`ï¼šé”™è¯¯ä¿¡æ¯ï¼ˆéœ€è¦å…³æ³¨ï¼‰

### ç›‘æ§ç«¯ç‚¹

- `/actuator/health` - å¥åº·æ£€æŸ¥
- `/actuator/metrics` - æŒ‡æ ‡ç›‘æ§
- `/actuator/prometheus` - Prometheus æ ¼å¼æŒ‡æ ‡

### æŠ¥å‘Šå­˜å‚¨

æŠ¥å‘Šé»˜è®¤å­˜å‚¨åœ¨ `./reports` ç›®å½•ä¸‹ï¼Œæ–‡ä»¶æ ¼å¼ï¼š
- `report_yyyyMMdd_HHmmss_reportId.json`

å»ºè®®å®šæœŸæ¸…ç†è¿‡æœŸæŠ¥å‘Šï¼š

```bash
# æ¸…ç†30å¤©å‰çš„æŠ¥å‘Š
find ./reports -name "report_*.json" -mtime +30 -delete
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **LLM API è°ƒç”¨å¤±è´¥**
   - æ£€æŸ¥ API Key é…ç½®
   - éªŒè¯ç½‘ç»œè¿æ¥
   - æŸ¥çœ‹ API é™åˆ¶å’Œä½™é¢

2. **æ•°æ®åº“è¿æ¥å¤±è´¥**
   - æ£€æŸ¥ ClickHouse æœåŠ¡çŠ¶æ€
   - éªŒè¯è¿æ¥é…ç½®
   - ç¡®è®¤æ•°æ®åº“æƒé™

3. **æŠ¥å‘Šç”Ÿæˆå¤±è´¥**
   - æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
   - éªŒè¯æ•°æ®æ ¼å¼
   - æŸ¥çœ‹é”™è¯¯æ—¥å¿—

### è°ƒè¯•æ¨¡å¼

å¯ç”¨è°ƒè¯•æ—¥å¿—ï¼š

```yaml
logging:
  level:
    com.o11y.ai: DEBUG
```

## æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–

- ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•
- ä½¿ç”¨åˆé€‚çš„æ—¶é—´åˆ†åŒºç­–ç•¥
- é™åˆ¶æŸ¥è¯¢æ—¶é—´èŒƒå›´

### LLM è°ƒç”¨ä¼˜åŒ–

- ä½¿ç”¨è¿æ¥æ± 
- å®ç°é‡è¯•æœºåˆ¶
- ç¼“å­˜ç›¸ä¼¼æŸ¥è¯¢ç»“æœ
- å¼‚æ­¥å¤„ç†é•¿æ—¶é—´åˆ†æ

### å†…å­˜ç®¡ç†

```bash
# è°ƒæ•´ JVM å‚æ•°
java -Xmx2g -Xms1g -XX:+UseG1GC -jar ai-analysis-module.jar
```

## è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. å‘èµ· Pull Request

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ LICENSE æ–‡ä»¶ã€‚
